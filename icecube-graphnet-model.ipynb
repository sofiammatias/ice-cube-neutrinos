{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sofiamatias/icecube-graphnet-model?scriptVersionId=125086449\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Graph Neural Networks on Competition Data\n\n<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/graphnet-team/graphnet/main/assets/identity/graphnet-logo-and-wordmark.png\" width=\"600\" height=\"600\" />\n\nThis notebook is a copy of graphnet-example and contains everything needed to tinker with GraphNeT on the competition data. It includes\n\n1. Installation instructions for GraphNeT\n\n2. Code for data conversion \n\n3. Snippets for training dynedge similarly to whats shown in the JINST paper\n\n4. A pre-trained dynedge on batch 1 to 50.\n\n5. Snippets for inference and evaluation of results.","metadata":{}},{"cell_type":"markdown","source":"## Installing GraphNeT\n\nYou can find the official installation instructions for GraphNeT [here](https://github.com/graphnet-team/graphnet#gear--install)\nThis code contains a few extra steps to get the library installed in a Kaggle notebook. It will copy a recent version of graphnet and it's dependencies to the working disk and install the GPU version of GraphNeT in /kaggle/working/software.","metadata":{}},{"cell_type":"code","source":"# Move software to working disk\n!rm  -r software\n!scp -r /kaggle/input/graphnet-and-dependencies/software .\n\n# Install dependencies\n!pip install /kaggle/working/software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/working/software/dependencies/torch_geometric-2.0.4.tar.gz\n\n# Install GraphNeT\n!cd software/graphnet;pip install --no-index --find-links=\"/kaggle/working/software/dependencies\" -e .[torch]\n\n# Append to PATH\nimport sys\nsys.path.append('/kaggle/working/software/graphnet/src')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-09T00:00:09.777309Z","iopub.execute_input":"2023-04-09T00:00:09.777767Z","iopub.status.idle":"2023-04-09T00:03:12.929258Z","shell.execute_reply.started":"2023-04-09T00:00:09.777676Z","shell.execute_reply":"2023-04-09T00:03:12.92778Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"rm: cannot remove 'software': No such file or directory\nProcessing ./software/dependencies/torch-1.11.0+cu115-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0+cu115) (4.1.1)\nInstalling collected packages: torch\n  Attempting uninstall: torch\n    Found existing installation: torch 1.11.0+cpu\n    Uninstalling torch-1.11.0+cpu:\n      Successfully uninstalled torch-1.11.0+cpu\nSuccessfully installed torch-1.11.0+cu115\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.0.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==0.6.13) (1.7.3)\nRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==0.6.13) (1.21.6)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing ./software/dependencies/torch_geometric-2.0.4.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.7.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.3.5)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (2.28.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric==2.0.4) (1.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric==2.0.4) (2.1.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric==2.0.4) (2022.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric==2.0.4) (2022.12.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric==2.0.4) (1.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->torch-geometric==2.0.4) (1.15.0)\nBuilding wheels for collected packages: torch-geometric\n  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=e4d259d97a4ea20522bab9308d94d616072b911ecb87910e41b20ca2bc918fa9\n  Stored in directory: /root/.cache/pip/wheels/c0/33/a3/07aa146f758cd91ebee36268011873ae31c2cfc59dec089e04\nSuccessfully built torch-geometric\nInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/working/software/dependencies\nObtaining file:///kaggle/working/software/graphnet\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hProcessing /kaggle/working/software/dependencies/awkward-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\nRequirement already satisfied: colorlog>=6.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (6.7.0)\nProcessing /kaggle/working/software/dependencies/ConfigUpdater-3.1.1-py2.py3-none-any.whl\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.3.6)\nRequirement already satisfied: matplotlib>=3.5 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (3.5.2)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.21.6)\nRequirement already satisfied: pandas>=1.3 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.3.5)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (8.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.8.2)\nProcessing /kaggle/working/software/dependencies/ruamel.yaml-0.17.21-py3-none-any.whl\nRequirement already satisfied: scikit_learn>=1.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.0.2)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.7.3)\nRequirement already satisfied: sqlalchemy>=1.4 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.4.39)\nProcessing /kaggle/working/software/dependencies/timer-0.2.2-py3-none-any.whl\nRequirement already satisfied: tqdm>=4.64 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (4.64.0)\nRequirement already satisfied: wandb>=0.12 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.12.21)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.11.0+cu115)\nRequirement already satisfied: torch-cluster>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.6.0)\nRequirement already satisfied: torch-scatter>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.9)\nRequirement already satisfied: torch-sparse>=0.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (0.6.13)\nRequirement already satisfied: torch-geometric>=2.0 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (2.0.4)\nRequirement already satisfied: pytorch-lightning>=1.6 in /opt/conda/lib/python3.7/site-packages (from graphnet==0.2.4+77.g57fd0f3) (1.9.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from awkward<2.0,>=1.8->graphnet==0.2.4+77.g57fd0f3) (59.8.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (9.1.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (4.33.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (23.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.5->graphnet==0.2.4+77.g57fd0f3) (1.4.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.3->graphnet==0.2.4+77.g57fd0f3) (2022.1)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (2023.1.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.1.1)\nRequirement already satisfied: lightning-utilities>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.5.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.11.0)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>=1.0->graphnet==0.2.4+77.g57fd0f3) (1.0.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (1.1.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (6.0.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.28.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.1.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.1.27)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (5.9.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.0.11)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.3.2)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.13.0)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.20.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.1.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (0.4.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (8.1.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (1.15.0)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (2.3)\nProcessing /kaggle/working/software/dependencies/ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (3.8.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (4.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.4->graphnet==0.2.4+77.g57fd0f3) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (3.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric>=2.0->graphnet==0.2.4+77.g57fd0f3) (2.1.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.3.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (6.0.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning>=1.6->graphnet==0.2.4+77.g57fd0f3) (4.0.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.12->graphnet==0.2.4+77.g57fd0f3) (3.0.5)\nInstalling collected packages: timer, ruamel.yaml.clib, awkward, ruamel.yaml, configupdater, graphnet\n  Running setup.py develop for graphnet\nSuccessfully installed awkward-1.8.0 configupdater-3.1.1 graphnet-0.2.4+77.g57fd0f3 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 timer-0.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import graphnet","metadata":{"execution":{"iopub.status.busy":"2023-04-09T00:07:25.82841Z","iopub.execute_input":"2023-04-09T00:07:25.829673Z","iopub.status.idle":"2023-04-09T00:07:25.838386Z","shell.execute_reply.started":"2023-04-09T00:07:25.829622Z","shell.execute_reply":"2023-04-09T00:07:25.836963Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Converting The Parquet Files to SQLite\n\nThe majority of functionality is tied to the SQLite data format. Therefore, to make GraphNeT compatible with the data provided in this competition, a small converter was included that reads a selection of batch_id's and writes them to a single database. The database will contain two tables\n\n* meta_table  : Contains the information associated with train_meta_data.parquet\n* pulse_table : Contains the information associated with the batch_n.parquet files, including detector geometry.\n\nboth tables are indexed according to *event_id* for efficiency. This will allow to extract information from the databases by simply writing\n\n```python \nimport pandas as pd\nimport sqlite3\n\nmy_event_id = 32\nmy_database = '/kaggle/working/sqlite/batch_01.db'\n\nwith sqlite3.connect(my_database) as conn:\n    # extracts meta data for event\n    meta_query = f'SELECT * FROM meta_table WHERE event_id ={my_event_id}'\n    meta_data = pd.read_sql(meta_query,conn)\n    \n    # extracts pulses / detector response for event\n    pulse_query = f'SELECT * FROM pulse_table WHERE event_id ={my_event_id}'\n    pulse_data = pd.read_sql(query,conn)\n```\n\nThe upside of the SQLite format is that you only have the events you want in memory. Downside is ... the conversion :-) \n","metadata":{}},{"cell_type":"code","source":"import pyarrow.parquet as pq\nimport sqlite3\nimport pandas as pd\nimport sqlalchemy\nfrom tqdm import tqdm\nimport os\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\n\nfrom graphnet.data.sqlite.sqlite_utilities import create_table\n\ndef load_input(meta_batch: pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n        \"\"\"\n        Will load the corresponding detector readings associated with the meta data batch.\n        \"\"\"\n        batch_id = pd.unique(meta_batch['batch_id'])\n\n        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n        \n        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n        sensor_positions.index = detector_readings.index\n\n        for column in sensor_positions.columns:\n            if column not in detector_readings.columns:\n                detector_readings[column] = sensor_positions[column]\n\n        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n        return detector_readings.reset_index()\n\ndef add_to_table(database_path: str,\n                      df: pd.DataFrame,\n                      table_name:  str,\n                      is_primary_key: bool,\n                      ) -> None:\n    \"\"\"Writes meta data to sqlite table. \n\n    Args:\n        database_path (str): the path to the database file.\n        df (pd.DataFrame): the dataframe that is being written to table.\n        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n    \"\"\"\n    try:\n        create_table(   columns=  df.columns,\n                        database_path = database_path, \n                        table_name = table_name,\n                        integer_primary_key= is_primary_key,\n                        index_column = 'event_id')\n    except sqlite3.OperationalError as e:\n        if 'already exists' in str(e):\n            pass\n        else:\n            raise e\n    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n    engine.dispose()\n    return\n\ndef convert_to_sqlite(meta_data_path: str,\n                      database_path: str,\n                      input_data_folder: str,\n                      batch_size: int = 20000,\n                      batch_ids: Optional[List[int]] = None,) -> None:\n    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n\n    Args:\n        meta_data_path (str): Path to the meta data file.\n        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n        input_data_folder (str): folder containing the parquet input files.\n        batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n    \"\"\"\n    if batch_ids is None:\n        batch_ids = np.arange(1,661,1).to_list()\n    else:\n        assert isinstance(batch_ids,list), \"Variable 'batch_ids' must be list.\"\n    if not database_path.endswith('.db'):\n        database_path = database_path+'.db'\n    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n    batch_id = 1\n    converted_batches = []\n    progress_bar = tqdm(total = len(batch_ids))\n    for meta_data_batch in tqdm(meta_data_iter):\n        if batch_id in batch_ids:\n            meta_data_batch  = meta_data_batch.to_pandas()\n            add_to_table(database_path = database_path,\n                        df = meta_data_batch,\n                        table_name='meta_table',\n                        is_primary_key= True)\n            pulses = load_input(meta_batch=meta_data_batch, input_data_folder= input_data_folder)\n            del meta_data_batch # memory\n            add_to_table(database_path = database_path,\n                        df = pulses,\n                        table_name='pulse_table',\n                        is_primary_key= False)\n            del pulses # memory\n            progress_bar.update(1)\n            converted_batches.append(batch_id)\n        batch_id +=1\n        if len(batch_ids) == len(converted_batches):\n            break\n    progress_bar.close()\n    del meta_data_iter # memory\n    print(f'Conversion Complete!. Database available at\\n {database_path}')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T00:07:31.568751Z","iopub.execute_input":"2023-04-09T00:07:31.569241Z","iopub.status.idle":"2023-04-09T00:07:31.595951Z","shell.execute_reply.started":"2023-04-09T00:07:31.569204Z","shell.execute_reply":"2023-04-09T00:07:31.595104Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This notebook comes with both batch 1 and 51 converted to sqlite databases, so you don't have to convert them yourself. They were produced by running the following code:\n\n```python \ninput_data_folder = '/kaggle/input/icecube-neutrinos-in-deep-ice/train'\ngeometry_table = pd.read_csv('/kaggle/input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv')\nmeta_data_path = '/kaggle/input/icecube-neutrinos-in-deep-ice/train_meta.parquet'\n\n#batch_1\ndatabase_path = '/kagge/working/batch_1'\nconvert_to_sqlite(meta_data_path,\n                  database_path=database_path,\n                  input_data_folder=input_data_folder,\n                  batch_ids = [1])\n\n#batch_51\ndatabase_path = '/kagge/working/batch_51'\nconvert_to_sqlite(meta_data_path,\n                  database_path=database_path,\n                  input_data_folder=input_data_folder,\n                  batch_ids = [51])\n```\n\nYou can convert multiple batches into a single database by adjusting *batch_id*. Notice that the compression of SQLite is far inferiour to parquet. The entire competition dataset will take up more than 1.5T of disk space in SQLite.\n\nInstead of producing these databases again, the next cell will copy them into /kaggle/working which is a faster disk.","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/batch-1/batch_1.db .\n!cp /kaggle/input/batch-51/batch_51.db .","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:33:29.469852Z","iopub.execute_input":"2023-02-03T08:33:29.470673Z","iopub.status.idle":"2023-02-03T08:34:33.177122Z","shell.execute_reply.started":"2023-02-03T08:33:29.47063Z","shell.execute_reply":"2023-02-03T08:34:33.175468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting train batches in a SQLite database","metadata":{}},{"cell_type":"code","source":"input_data_folder = '/kaggle/input/icecube-neutrinos-in-deep-ice/train'\ngeometry_table = pd.read_csv('/kaggle/input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv')\nmeta_data_path = '/kaggle/input/icecube-neutrinos-in-deep-ice/train_meta.parquet'\n\n#batch_295\ndatabase_path = '/kaggle/working/batch_295'\nconvert_to_sqlite(meta_data_path,\n                  database_path=database_path,\n                  input_data_folder=input_data_folder,\n                  batch_ids = [295])","metadata":{"execution":{"iopub.status.busy":"2023-04-09T00:23:57.52341Z","iopub.execute_input":"2023-04-09T00:23:57.525448Z","iopub.status.idle":"2023-04-09T00:34:18.168813Z","shell.execute_reply.started":"2023-04-09T00:23:57.525379Z","shell.execute_reply":"2023-04-09T00:34:18.167504Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [10:20<00:00, 620.56s/it]","output_type":"stream"},{"name":"stdout","text":"Conversion Complete!. Database available at\n /kaggle/working/batch_295.db\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Defining A Selection\nThe [SQLiteDataset class](https://github.com/graphnet-team/graphnet/blob/main/src/graphnet/data/sqlite/sqlite_dataset.py) is essentially a PyTorch Dataset (read more [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)) where the __get_item__ function extracts a single event at a time from the specified database. If a so-called *selection* is specified, only events in the selection are used for the dataset - that allows us to sub-sample the dataset for training.\n\nThe following few cells introduce a simple selection based on the number of pulses. This selection will then be used for training a GNN later.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef make_selection(df: pd.DataFrame, pulse_threshold: int = 200) -> None:\n    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default. \"\"\"\n    n_events = np.arange(0, len(df),1)\n    train_selection, validate_selection = train_test_split(n_events, \n                                                           shuffle=True, \n                                                           random_state = 42, \n                                                           test_size=0.20) \n    df['train'] = 0\n    df['validate'] = 0\n    \n    df['train'][train_selection] = 1\n    df['validate'][validate_selection] = 1\n    \n    assert len(train_selection) == sum(df['train'])\n    assert len(validate_selection) == sum(df['validate'])\n\n    #Remove events with large pulses from training and validation sample (memory)\n    df['train'][df['n_pulses']> pulse_threshold] = 0\n    df['validate'][df['n_pulses']> pulse_threshold] = 0\n    \n    for selection in ['train', 'validate']:\n        df.loc[df[selection] == 1, :].to_csv(f'{selection}_selection_max_{pulse_threshold}_pulses.csv')\n    return\n\ndef get_number_of_pulses(db: str, event_id: int, pulsemap: str) -> int:\n    with sqlite3.connect(db) as con:\n        query = f'select event_id from {pulsemap} where event_id = {event_id} limit 20000'\n        data = con.execute(query).fetchall()\n    return len(data)\n\ndef count_pulses(database: str, pulsemap: str) -> pd.DataFrame:\n    \"\"\" Will count the number of pulses in each event and return a single dataframe that contains counts for each event_id.\"\"\"\n    with sqlite3.connect(database) as con:\n        query = 'select event_id from meta_table'\n        events = pd.read_sql(query,con)\n    counts = {'event_id': [],\n              'n_pulses': []}\n    for event_id in tqdm(events['event_id']):\n        a = get_number_of_pulses(database, event_id, pulsemap)\n        counts['event_id'].append(event_id)\n        counts['n_pulses'].append(a)\n    df = pd.DataFrame(counts)\n    df.to_csv('counts.csv')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cell below will produce three csv files; training and validation selections, and a third files called *counts.csv*. ","metadata":{}},{"cell_type":"code","source":"pulsemap = 'pulse_table'\ndatabase = '/kaggle/working/batch_295.db'\n\ndf = count_pulses(database, pulsemap)\nmake_selection(df = df, pulse_threshold =  200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nfig = plt.figure(figsize=(6,4), constrained_layout = True)\nplt.hist(df['n_pulses'], histtype = 'step', label = 'batch_1', bins = np.arange(0,400,1))\nplt.xlabel('# of Pulses', size = 15);\nplt.xticks(size = 15);\nplt.yticks(size = 15);\nplt.plot(np.repeat(200,2), [0, 4000], label = f'Selection\\n{np.round((sum(df[\"n_pulses\"]<= 200)/len(df))*100, 1)} % pass' ) \nplt.legend(frameon = False, fontsize = 15);","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:49.836752Z","iopub.execute_input":"2023-02-03T08:35:49.837371Z","iopub.status.idle":"2023-02-03T08:35:50.25645Z","shell.execute_reply.started":"2023-02-03T08:35:49.837324Z","shell.execute_reply":"2023-02-03T08:35:50.255463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Event with highest number of pulses counted: {df[\"n_pulses\"].max()}')","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:50.257832Z","iopub.execute_input":"2023-02-03T08:35:50.25885Z","iopub.status.idle":"2023-02-03T08:35:50.266848Z","shell.execute_reply.started":"2023-02-03T08:35:50.258772Z","shell.execute_reply":"2023-02-03T08:35:50.265678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As visible from the plot, most events have less or equal to 200 pulses. The print message shows us that the outliers can have tens of thousands of pulses (20000 is the max we let it count in the code above). In principle, one could train the GNN without descrimination on this collection of events, but events with large pulse counts will make the memory usuage volatile and force a low batch size. Therefore (and feel free to challenge this) this notebook will train on events with 200 or less pulses.  ","metadata":{}},{"cell_type":"markdown","source":"## Training DynEdge\n\nNow that both database and selection is ready, everything is in place to begin training. DynEdge is a GNN implemented in GraphNeT - it represents IceCube events as 3D point clouds and leverages techniques from segmentation analysis in computer vision to reconstruct events. You can find technical details on the model in [this paper](https://iopscience.iop.org/article/10.1088/1748-0221/17/11/P11003). The model and training configuration shown below is nearly identical to what's presented in the paper. Note that this configuration was originally meant for low energy, so it's possible that some adjustments might improve performance.","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning.callbacks import EarlyStopping\nfrom torch.optim.adam import Adam\nfrom graphnet.data.constants import FEATURES, TRUTH\nfrom graphnet.models import StandardModel\nfrom graphnet.models.detector.icecube import IceCubeKaggle\nfrom graphnet.models.gnn import DynEdge\nfrom graphnet.models.graph_builders import KNNGraphBuilder\nfrom graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\nfrom graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\nfrom graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\nfrom graphnet.training.labels import Direction\nfrom graphnet.training.utils import make_dataloader\nfrom graphnet.utilities.logging import get_logger\nfrom pytorch_lightning import Trainer\nimport pandas as pd\n\nlogger = get_logger()\n\ndef build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n    \"\"\"Builds GNN from config\"\"\"\n    # Building model\n    detector = IceCubeKaggle(\n        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n    )\n    gnn = DynEdge(\n        nb_inputs=detector.nb_outputs,\n        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n    )\n\n    if config[\"target\"] == 'direction':\n        task = DirectionReconstructionWithKappa(\n            hidden_size=gnn.nb_outputs,\n            target_labels=config[\"target\"],\n            loss_function=VonMisesFisher3DLoss(),\n        )\n        prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n        additional_attributes = ['zenith', 'azimuth', 'event_id']\n\n    model = StandardModel(\n        detector=detector,\n        gnn=gnn,\n        tasks=[task],\n        optimizer_class=Adam,\n        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n        scheduler_class=PiecewiseLinearLR,\n        scheduler_kwargs={\n            \"milestones\": [\n                0,\n                len(train_dataloader) / 2,\n                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n            ],\n            \"factors\": [1e-02, 1, 1e-02],\n        },\n        scheduler_config={\n            \"interval\": \"step\",\n        },\n    )\n    model.prediction_columns = prediction_columns\n    model.additional_attributes = additional_attributes\n    \n    return model\n\ndef load_pretrained_model(config: Dict[str,Any], state_dict_path: str = '/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth') -> StandardModel:\n    train_dataloader, _ = make_dataloaders(config = config)\n    model = build_model(config = config, \n                        train_dataloader = train_dataloader)\n    model.load_state_dict(state_dict_path)\n    model.prediction_columns = [config[\"target\"] + \"_x\", \n                              config[\"target\"] + \"_y\", \n                              config[\"target\"] + \"_z\", \n                              config[\"target\"] + \"_kappa\" ]\n    model.additional_attributes = ['zenith', 'azimuth', 'event_id']\n    return model\n\ndef make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n    train_dataloader = make_dataloader(db = config['path'],\n                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist(),\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = True,\n                                            labels = {'direction': Direction()},\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                            )\n    \n    validate_dataloader = make_dataloader(db = config['path'],\n                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist(),\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = False,\n                                            labels = {'direction': Direction()},\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                          \n                                            )\n    return train_dataloader, validate_dataloader\n\ndef train_dynedge_from_scratch(config: Dict[str, Any]) -> StandardModel:\n    \"\"\"Builds and trains GNN according to config.\"\"\"\n    logger.info(f\"features: {config['features']}\")\n    logger.info(f\"truth: {config['truth']}\")\n    \n    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n\n    train_dataloader, validate_dataloader = make_dataloaders(config = config)\n\n    model = build_model(config, train_dataloader)\n\n    # Training model\n    callbacks = [\n        EarlyStopping(\n            monitor=\"val_loss\",\n            patience=config[\"early_stopping_patience\"],\n        ),\n        ProgressBar(),\n    ]\n\n    model.fit(\n        train_dataloader,\n        validate_dataloader,\n        callbacks=callbacks,\n        **config[\"fit\"],\n    )\n    return model\n\ndef inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n    # Make Dataloader\n    test_dataloader = make_dataloader(db = config['inference_database_path'],\n                                            selection = None, # Entire database\n                                            pulsemaps = config['pulsemap'],\n                                            features = features,\n                                            truth = truth,\n                                            batch_size = config['batch_size'],\n                                            num_workers = config['num_workers'],\n                                            shuffle = False,\n                                            labels = {'direction': Direction()},\n                                            index_column = config['index_column'],\n                                            truth_table = config['truth_table'],\n                                            )\n    \n    # Get predictions\n    results = model.predict_as_dataframe(\n        gpus = [0],\n        dataloader = test_dataloader,\n        prediction_columns=model.prediction_columns,\n        additional_attributes=model.additional_attributes,\n    )\n    # Save predictions and model to file\n    archive = os.path.join(config['base_dir'], \"train_model_without_configs\")\n    run_name = f\"dynedge_{config['target']}_{config['run_name_tag']}\"\n    db_name = config['path'].split(\"/\")[-1].split(\".\")[0]\n    path = os.path.join(archive, db_name, run_name)\n    logger.info(f\"Writing results to {path}\")\n    os.makedirs(path, exist_ok=True)\n\n    results.to_csv(f\"{path}/results.csv\")\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:50.268509Z","iopub.execute_input":"2023-02-03T08:35:50.269123Z","iopub.status.idle":"2023-02-03T08:35:51.121812Z","shell.execute_reply.started":"2023-02-03T08:35:50.269085Z","shell.execute_reply":"2023-02-03T08:35:51.120342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nfeatures = FEATURES.KAGGLE\ntruth = TRUTH.KAGGLE\n\n# Configuration\nconfig = {\n        \"path\": '/kaggle/working/batch_1.db',\n        \"inference_database_path\": '/kaggle/working/batch_51.db',\n        \"pulsemap\": 'pulse_table',\n        \"truth_table\": 'meta_table',\n        \"features\": features,\n        \"truth\": truth,\n        \"index_column\": 'event_id',\n        \"run_name_tag\": 'my_example',\n        \"batch_size\": 200,\n        \"num_workers\": 2,\n        \"target\": 'direction',\n        \"early_stopping_patience\": 5,\n        \"fit\": {\n                \"max_epochs\": 50,\n                \"gpus\": [0],\n                \"distribution_strategy\": None,\n                },\n        'train_selection': '/kaggle/working/train_selection_max_200_pulses.csv',\n        'validate_selection': '/kaggle/working/validate_selection_max_200_pulses.csv',\n        'test_selection': None,\n        'base_dir': 'training'\n}","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:51.127064Z","iopub.execute_input":"2023-02-03T08:35:51.127385Z","iopub.status.idle":"2023-02-03T08:35:51.137833Z","shell.execute_reply.started":"2023-02-03T08:35:51.127354Z","shell.execute_reply":"2023-02-03T08:35:51.136719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell below, you can choose between training dynedge from scratch on the batch_1 database or loading in a pretrained model that has trained on batches 1 to 50.","metadata":{}},{"cell_type":"code","source":"# Train from scratch (slow) - remember to save it!\n#model = train_dynedge_from_scratch(config = config)\n\n# Load state-dict from pre-trained model (faster)\nmodel = load_pretrained_model(config = config)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:51.139859Z","iopub.execute_input":"2023-02-03T08:35:51.140351Z","iopub.status.idle":"2023-02-03T08:35:51.806054Z","shell.execute_reply.started":"2023-02-03T08:35:51.140312Z","shell.execute_reply":"2023-02-03T08:35:51.805023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference & Evaluation\n\nWith a trained model loaded into memory, we can now apply the model to batch_51. The following cells will start inference (or load in a csv with predictions, if you're in a hurry) and plot the results. ","metadata":{}},{"cell_type":"code","source":"# Inference\nresults = inference(model, config)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:35:51.807346Z","iopub.execute_input":"2023-02-03T08:35:51.807746Z","iopub.status.idle":"2023-02-03T08:46:55.296296Z","shell.execute_reply.started":"2023-02-03T08:35:51.807704Z","shell.execute_reply":"2023-02-03T08:46:55.295092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n    df['true_x'] = np.cos(df['azimuth']) * np.sin(df['zenith'])\n    df['true_y'] = np.sin(df['azimuth'])*np.sin(df['zenith'])\n    df['true_z'] = np.cos(df['zenith'])\n    return df\n\ndef calculate_angular_error(df : pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Calculates the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n    df['angular_error'] = np.arccos(df['true_x']*df['direction_x'] + df['true_y']*df['direction_y'] + df['true_z']*df['direction_z'])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:46:55.298831Z","iopub.execute_input":"2023-02-03T08:46:55.299249Z","iopub.status.idle":"2023-02-03T08:46:55.307103Z","shell.execute_reply.started":"2023-02-03T08:46:55.299205Z","shell.execute_reply":"2023-02-03T08:46:55.306088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = convert_to_3d(results)\nresults = calculate_angular_error(results)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:46:55.30843Z","iopub.execute_input":"2023-02-03T08:46:55.309457Z","iopub.status.idle":"2023-02-03T08:46:55.360992Z","shell.execute_reply.started":"2023-02-03T08:46:55.30942Z","shell.execute_reply":"2023-02-03T08:46:55.359991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (6,6))\nplt.hist(results['angular_error'], \n         bins = np.arange(0,np.pi*2, 0.05), \n         histtype = 'step', \n         label = f'mean angular error: {np.round(results[\"angular_error\"].mean(),2)}')\nplt.xlabel('Angular Error [rad.]', size = 15)\nplt.ylabel('Counts', size = 15)\nplt.title('Angular Error Distribution (Batch 51)', size = 15)\nplt.legend(frameon = False, fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:56:19.735174Z","iopub.execute_input":"2023-02-03T08:56:19.735528Z","iopub.status.idle":"2023-02-03T08:56:19.981355Z","shell.execute_reply.started":"2023-02-03T08:56:19.735497Z","shell.execute_reply":"2023-02-03T08:56:19.980402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the pre-trained dynedge seems to perform quite well. Another interesting feature of the reconstruction is that dynedge (when coupled with the[ DirectionReconstructionWithKappa](https://github.com/graphnet-team/graphnet/blob/7e857562898ebebebc9a105159fd3d4eb4994aea/src/graphnet/models/task/reconstruction.py#L45) is that dynedge estimated *kappa* the concentration parameter from the vonMisesFisher distribution. Kappa is analogus to sigma via sigma = 1/sqrt(kappa), and the quality of the direction estimate should be highly correlated with this parameter. ","metadata":{}},{"cell_type":"code","source":"cut_threshold = 0.5\nfig = plt.figure(figsize = (6,6))\nplt.hist(results['angular_error'][1/np.sqrt(results['direction_kappa']) <= cut_threshold], \n         bins = np.arange(0,np.pi*2, 0.05), \n         histtype = 'step', \n         label = f'sigma <= {cut_threshold}: {np.round(results[\"angular_error\"][1/np.sqrt(results[\"direction_kappa\"]) <= cut_threshold].mean(),2)}')\n\nplt.hist(results['angular_error'][1/np.sqrt(results['direction_kappa']) > cut_threshold], \n         bins = np.arange(0,np.pi*2, 0.05), \n         histtype = 'step', \n         label = f'sigma > {cut_threshold}: {np.round(results[\"angular_error\"][1/np.sqrt(results[\"direction_kappa\"]) > cut_threshold].mean(),2)}')\nplt.xlabel('Angular Error [rad.]', size = 15)\nplt.ylabel('Counts', size = 15)\nplt.title('Angular Error Distribution (Batch 51)', size = 15)\nplt.legend(frameon = False, fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T08:56:24.975565Z","iopub.execute_input":"2023-02-03T08:56:24.97625Z","iopub.status.idle":"2023-02-03T08:56:25.236386Z","shell.execute_reply.started":"2023-02-03T08:56:24.976211Z","shell.execute_reply":"2023-02-03T08:56:25.235418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the variable can be used to distinguish \"good\" and \"bad\" reconstructions with some confidence. ","metadata":{}},{"cell_type":"markdown","source":"## A few hints for your neutrino data science journey!\n\n* The configuration of dynedge shown in this notebook is the so-called \"baseline\". It's not optimized for high energy neutrinos, so you might be able to squeeze out a bit more performance by tuning hyperparameters or making larger modifications; such as switching out the learning rate scheduler or choosing a different loss function, etc.\n\n* You can use the kappa variable to group events into different categories. Perhaps training a seperate reconstruction method for each performs better?\n\n* You may want to adjust the [ParquetDataset](https://github.com/graphnet-team/graphnet/blob/7e857562898ebebebc9a105159fd3d4eb4994aea/src/graphnet/data/parquet/parquet_dataset.py#L11) such that it works with the competition data. This would allow you to train / infer directly on the competition files (No conversion to sqlite needed). Feel free to contribute this to the repository!\n\n\nGood luck!","metadata":{}}]}